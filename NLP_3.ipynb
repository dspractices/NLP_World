{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b50c0df7-61cd-4ae1-8242-b1ab086f55f5",
   "metadata": {},
   "source": [
    "### Stemming & Lemmatization & POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8418dcc-ef20-4c62-b00f-9135063aa577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n",
      "generously | gener\n"
     ]
    }
   ],
   "source": [
    "# Perform Stemming with NLTK\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\", \"generously\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"|\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cebeae9-6423-4990-9beb-b8b2496645ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n",
      "generously | generous\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\", \"generously\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"|\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df6fab4e-266a-4884-a464-92ee6f94f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niya  |  Niya\n",
      "talked  |  talk\n",
      "for  |  for\n",
      "3  |  3\n",
      "hours  |  hour\n",
      "although  |  although\n",
      "talking  |  talk\n",
      "is  |  be\n",
      "n't  |  not\n",
      "her  |  her\n",
      "likes  |  like\n"
     ]
    }
   ],
   "source": [
    "# Perform @ Lemmatization with Spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Niya talked for 3 hours although talking isn't her likes\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af14c1b2-6270-43c7-b048-0f59dc087324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  eat\n",
      "adjustable  |  adjustable\n",
      "rafting  |  raft\n",
      "ability  |  ability\n",
      "meeting  |  meeting\n",
      "better  |  well\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e92c085-9184-46dd-8e55-5c91fcdcdb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70f2a2c3-926d-4bde-9c92-38327d8c3262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | bro\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brah\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94e43384-2f32-4c1a-923b-06118fc6743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "\n",
    "ar.add([[{\"TEXT\":\"Bro\", \"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "508bf16c-1c8b-4c6f-a2ee-9921bd7cbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  interjection\n",
      "!  |  PUNCT  |  punctuation\n",
      "Harry  |  PROPN  |  proper noun\n",
      "said  |  VERB  |  verb\n",
      ",  |  PUNCT  |  punctuation\n",
      "what  |  PRON  |  pronoun\n",
      "an  |  DET  |  determiner\n",
      "amazing  |  ADJ  |  adjective\n",
      "movie  |  NOUN  |  noun\n",
      "is  |  AUX  |  auxiliary\n",
      "Avenger  |  PROPN  |  proper noun\n",
      "!  |  PUNCT  |  punctuation\n"
     ]
    }
   ],
   "source": [
    "# POS\n",
    "doc = nlp(\"Wow! Harry said, what an amazing movie is Avenger!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2159de72-e591-481d-8ed5-2c6abf32da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  interjection  |  UH  |  interjection\n",
      "!  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
      "Harry  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "said  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
      ",  |  PUNCT  |  punctuation  |  ,  |  punctuation mark, comma\n",
      "what  |  PRON  |  pronoun  |  WP  |  wh-pronoun, personal\n",
      "an  |  DET  |  determiner  |  DT  |  determiner\n",
      "amazing  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "movie  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n",
      "is  |  AUX  |  auxiliary  |  VBZ  |  verb, 3rd person singular present\n",
      "Avenger  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "!  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# Tags\n",
    "doc = nlp(\"Wow! Harry said, what an amazing movie is Avenger!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ce6c2fb-fcaa-4a5f-a611-5b395ece9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove space, puncutations etc\n",
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"PUNCT\", \"X\"]:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df1b8eba-c414-455f-a495-6513fe83d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Wow, Harry, said, what, an, amazing, movie, is, Avenger]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7666e416-50f8-482c-b4f4-3244d4e8ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER - Named Entitity Recognization\n",
    "text = \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2117bb01-86be-40ff-9474-1f09b453debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographical location Names:  [India, Delhi, Gujarat, Tamilnadu, Pongal, Andhrapradesh, Assam, Bihar]\n"
     ]
    }
   ],
   "source": [
    "#list for storing all the names\n",
    "all_gpe_names = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "  if ent.label_ == 'GPE':     #checking the whether token belongs to entity \"GPE\" [Geographical location]\n",
    "    all_gpe_names.append(ent)\n",
    "\n",
    "#finally printing the results\n",
    "print(\"Geographical location Names: \", all_gpe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c268f5f9-5c8b-46b0-aa83-b20a600b6464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sachin Tendulkar PERSON\n",
      "24 April 1973 DATE\n",
      "Virat Kholi LOC\n",
      "5 November 1988 DATE\n",
      "Dhoni PERSON\n",
      "7 July 1981 DATE\n",
      "Ricky PERSON\n",
      "19 December 1974 DATE\n",
      "All Birth Dates:  [24 April 1973, 5 November 1988, 7 July 1981, 19 December 1974]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kholi was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
    "and finally Ricky ponting was born on 19 December 1974.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#list for storing all the dates\n",
    "all_birth_dates = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    if ent.label_ == 'DATE':     #checking the whether token belongs to entity \"DATE\" [Dates]\n",
    "        all_birth_dates.append(ent)\n",
    "\n",
    "#finally printing the results\n",
    "print(\"All Birth Dates: \", all_birth_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235bc3a-3599-4e28-8e9e-274f941f5865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
