{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3dff5b-40a0-4165-a027-024f1efd29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9454f7b-eece-4b2e-84c0-2619da8f67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65226af5-9314-40f2-895d-27bd606b8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) and computational linguistics that focuses on the \n",
    "interaction between computers and human languages. I do have $2 for buying burger.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc84a124-8766-43c1-b0a2-348cb6f1f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be612926-97df-42c4-adcd-7da280b40942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural  |  proper noun\n",
      "Language  |  proper noun\n",
      "Processing  |  proper noun\n",
      "(  |  punctuation\n",
      "NLP  |  proper noun\n",
      ")  |  punctuation\n",
      "is  |  auxiliary\n",
      "a  |  determiner\n",
      "subfield  |  noun\n",
      "of  |  adposition\n",
      "artificial  |  adjective\n",
      "intelligence  |  noun\n",
      "(  |  punctuation\n",
      "AI  |  proper noun\n",
      ")  |  punctuation\n",
      "and  |  coordinating conjunction\n",
      "computational  |  adjective\n",
      "linguistics  |  noun\n",
      "that  |  pronoun\n",
      "focuses  |  verb\n",
      "on  |  adposition\n",
      "the  |  determiner\n",
      "\n",
      "  |  space\n",
      "interaction  |  noun\n",
      "between  |  adposition\n",
      "computers  |  noun\n",
      "and  |  coordinating conjunction\n",
      "human  |  adjective\n",
      "languages  |  noun\n",
      ".  |  punctuation\n",
      "I  |  pronoun\n",
      "do  |  auxiliary\n",
      "have  |  verb\n",
      "$  |  symbol\n",
      "2  |  numeral\n",
      "for  |  adposition\n",
      "buying  |  verb\n",
      "burger  |  noun\n",
      ".  |  punctuation\n"
     ]
    }
   ],
   "source": [
    "#POS tags\n",
    "for token in dc:\n",
    "    print(token, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb455464-ba4a-4002-91cf-53dbbb707637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "filtered_tokens = []\n",
    "for token in dc:\n",
    "    if token.pos_ not in [\"SPACE\", \"PUNCT\", \"X\"]:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34e3dea7-2a47-49cf-8f84-3bf8930fd29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Natural,\n",
       " Language,\n",
       " Processing,\n",
       " NLP,\n",
       " is,\n",
       " a,\n",
       " subfield,\n",
       " of,\n",
       " artificial,\n",
       " intelligence,\n",
       " AI,\n",
       " and,\n",
       " computational,\n",
       " linguistics,\n",
       " that,\n",
       " focuses,\n",
       " on,\n",
       " the,\n",
       " interaction,\n",
       " between,\n",
       " computers,\n",
       " and,\n",
       " human,\n",
       " languages,\n",
       " I,\n",
       " do,\n",
       " have,\n",
       " $,\n",
       " 2,\n",
       " for,\n",
       " buying,\n",
       " burger]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99580eae-a658-41f8-8c25-6b270f413543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Language ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Processing ==> index:  2 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "( ==> index:  3 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "NLP ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ") ==> index:  5 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "is ==> index:  6 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "a ==> index:  7 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "subfield ==> index:  8 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "of ==> index:  9 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "artificial ==> index:  10 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "intelligence ==> index:  11 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "( ==> index:  12 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "AI ==> index:  13 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ") ==> index:  14 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "and ==> index:  15 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "computational ==> index:  16 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "linguistics ==> index:  17 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "that ==> index:  18 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "focuses ==> index:  19 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "on ==> index:  20 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "the ==> index:  21 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "\n",
      " ==> index:  22 is_alpha: False is_punct: False like_num: False is_currency: False\n",
      "interaction ==> index:  23 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "between ==> index:  24 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "computers ==> index:  25 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "and ==> index:  26 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "human ==> index:  27 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "languages ==> index:  28 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ". ==> index:  29 is_alpha: False is_punct: True like_num: False is_currency: False\n",
      "I ==> index:  30 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "do ==> index:  31 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "have ==> index:  32 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "$ ==> index:  33 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "2 ==> index:  34 is_alpha: False is_punct: False like_num: True is_currency: False\n",
      "for ==> index:  35 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "buying ==> index:  36 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "burger ==> index:  37 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ". ==> index:  38 is_alpha: False is_punct: True like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in dc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha, \n",
    "          \"is_punct:\", token.is_punct, \n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1b3799c-7f5a-47a7-af36-cf776ce9673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce8639a1-62ac-4651-ab08-a4d2e48ebbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Nijan.',\n",
       " 'I love building ML models and analyzing.',\n",
       " 'This is NLP WORLD!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"I am Nijan. I love building ML models and analyzing. This is NLP WORLD!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7579cbfc-5bed-4356-9c5a-bd9f4cc0b489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Nijan',\n",
       " '.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'building',\n",
       " 'ML',\n",
       " 'models',\n",
       " 'and',\n",
       " 'analyzing',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'NLP',\n",
       " 'WORLD',\n",
       " '!']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I am Nijan. I love building ML models and analyzing. This is NLP WORLD!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a8172fd-5e5f-40e0-847a-84fa326edba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('students.txt', 'r') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fa6bf71-f1cd-4d65-9abf-4ce863546e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "713bbb65-262e-40f7-9f75-5b707b1504ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8520df83-4886-42bf-9d28-3adb7eee8149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dayton high school, 8th grade students information\n",
       " ==================================================\n",
       " \n",
       " Name\tbirth day   \temail\n",
       " -----\t------------\t------\n",
       " Virat   5 June, 1882    virat@kohli.com\n",
       " Maria\t12 April, 2001  maria@sharapova.com\n",
       " Serena  24 June, 1998   serena@williams.com \n",
       " Joe      1 May, 1997    joe@root.com\n",
       " \n",
       " \n",
       " "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5aa3ef86-83c5-4d58-8ef7-5f18d7e9787f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f531d5b-a86c-4e6d-84de-af9a90316594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"|\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad550c-3a9f-41df-8c77-2b67d4d583a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
